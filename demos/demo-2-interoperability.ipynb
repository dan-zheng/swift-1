{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1IIvuOcU3nO"
   },
   "source": [
    "# 语言交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLJOEl93eyn7"
   },
   "source": [
    "## 直接导入 Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZRlD4utdPuX"
   },
   "outputs": [],
   "source": [
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6DdXZTcesgD"
   },
   "outputs": [],
   "source": [
    "let np = Python.import(\"numpy\")\n",
    "let plt = Python.import(\"matplotlib.pyplot\")\n",
    "\n",
    "// Also enable Jupyter's display capabilities.\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4FLCl5peuL9"
   },
   "outputs": [],
   "source": [
    "let x = np.linspace(0, 10, 100)\n",
    "\n",
    "plt.plot(x, np.sin(x))\n",
    "plt.plot(x, np.cos(x))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wY8zcF8YxyEe"
   },
   "source": [
    "## 用 Python 玩 AI 游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtlPQ2qh_4gZ"
   },
   "outputs": [],
   "source": [
    "let gym = Python.import(\"gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DWYozGH_8f3"
   },
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "\n",
    "/// Model parameters and hyperparameters.\n",
    "let hiddenSize = 128\n",
    "let batchSize = 16\n",
    "/// Controls the amount of good/long episodes to retain for training.\n",
    "let percentile = 70\n",
    "\n",
    "/// An episode is a list of steps, where each step records the observation from\n",
    "/// env and the action taken. They will serve respectively as the input and\n",
    "/// target (label) of the neural net training.\n",
    "struct Episode {\n",
    "    struct Step {\n",
    "        let observation: Tensor<Float>\n",
    "        let action: Int32\n",
    "    }\n",
    "\n",
    "    let steps: [Step]\n",
    "    let reward: Float\n",
    "}\n",
    "\n",
    "/// Filtering out bad/short episodes before we feed them as neural net training data.\n",
    "func filteringBatch(\n",
    "  episodes: [Episode],\n",
    "  actionCount: Int\n",
    ") -> (input: Tensor<Float>, target: Tensor<Float>, episodeCount: Int, meanReward: Float) {\n",
    "    let rewards = episodes.map { $0.reward }\n",
    "    let rewardBound = Float(np.percentile(rewards, percentile))!\n",
    "    print(\"rewardBound = \\(rewardBound)\")\n",
    "\n",
    "    var input = Tensor<Float>(0.0)\n",
    "    var target = Tensor<Float>(0.0)\n",
    "    var totalReward: Float = 0.0\n",
    "\n",
    "    var retainedEpisodeCount = 0\n",
    "    for episode in episodes {\n",
    "        if episode.reward < rewardBound {\n",
    "            continue\n",
    "        }\n",
    "\n",
    "        let observationTensor = Tensor<Float>(episode.steps.map { $0.observation })\n",
    "        let actionTensor = Tensor<Int32>(episode.steps.map { $0.action })\n",
    "        let oneHotLabels = Tensor<Float>(oneHotAtIndices: actionTensor, depth: actionCount)\n",
    "\n",
    "        if retainedEpisodeCount == 0 {\n",
    "            input = observationTensor\n",
    "            target = oneHotLabels\n",
    "        } else {\n",
    "            input = input.concatenated(with: observationTensor)\n",
    "            target = target.concatenated(with: oneHotLabels)\n",
    "        }\n",
    "\n",
    "        totalReward += episode.reward\n",
    "        retainedEpisodeCount += 1\n",
    "    }\n",
    "\n",
    "    return (input, target, retainedEpisodeCount, totalReward / Float(retainedEpisodeCount))\n",
    "}\n",
    "\n",
    "struct CartPoleEnvironment {\n",
    "    let env: PythonObject\n",
    "    func reset() -> Tensor<Float> {\n",
    "        return Tensor<Float>(Tensor<Double>(numpy: env.reset())!)\n",
    "    }\n",
    "    func step(_ action: Int32) -> (Tensor<Float>, Float, Bool) {\n",
    "        let (nextObservation, reward, isDone, _) = env.step(Int(action)).tuple4\n",
    "        return (\n",
    "            Tensor<Float>(Tensor<Double>(numpy: nextObservation)!),\n",
    "            Float(reward)!, Bool(isDone)!)\n",
    "    }\n",
    "}\n",
    "\n",
    "extension Tensor where Scalar: TensorFlowFloatingPoint {\n",
    "    func categorical(samples: Int) -> Tensor<Int32> {\n",
    "        let logits = self.rank == 1 ? self.reshaped(to: [1, self.shape[0]]) : self\n",
    "        return Raw.multinomial(\n",
    "            logits: logits,\n",
    "            numSamples: Tensor<Int32>(Int32(samples)))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnT7vamXWFnN"
   },
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2gbhNcEWJ4B"
   },
   "outputs": [],
   "source": [
    "/// A simple two layer dense neural net.\n",
    "struct Net: Layer {\n",
    "    typealias Input = Tensor<Float>\n",
    "    typealias Output = Tensor<Float>\n",
    "\n",
    "    var l1, l2: Dense<Float>\n",
    "\n",
    "    init(observationSize: Int, hiddenSize: Int, actionCount: Int) {\n",
    "        l1 = Dense<Float>(inputSize: observationSize, outputSize: hiddenSize, activation: relu)\n",
    "        l2 = Dense<Float>(inputSize: hiddenSize, outputSize: actionCount)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: Input) -> Output {\n",
    "        return input.sequenced(through: l1, l2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddpadylwWKoL"
   },
   "source": [
    "### 观察与行动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNj_oYbBWv4a"
   },
   "outputs": [],
   "source": [
    "func nextBatch(env: CartPoleEnvironment, net: Net, batchSize: Int, actionCount: Int) -> [Episode] {\n",
    "    var observation = env.reset()\n",
    "\n",
    "    var episodes: [Episode] = []\n",
    "\n",
    "    // Build up a batch of observations and actions.\n",
    "    for _ in 0..<batchSize {\n",
    "        var steps: [Episode.Step] = []\n",
    "        var episodeReward: Float = 0.0\n",
    "\n",
    "        // This loop runs one episode.\n",
    "        while true {\n",
    "            let action = net(observation.reshaped(to: [1, 4])).categorical(samples: 1).scalarized()\n",
    "            let (nextObservation, reward, isDone) = env.step(action)\n",
    "            steps.append(Episode.Step(observation: observation, action: action))\n",
    "\n",
    "            episodeReward += reward\n",
    "\n",
    "            if isDone == true {\n",
    "                episodes.append(Episode(steps: steps, reward: episodeReward))\n",
    "                observation = env.reset()\n",
    "                break\n",
    "            } else {\n",
    "                observation = nextObservation\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return episodes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBrWcVm9Yn2w"
   },
   "source": [
    "### 设置游戏环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVdddhwUYpnE"
   },
   "outputs": [],
   "source": [
    "let env = CartPoleEnvironment(env: gym.make(\"CartPole-v0\"))\n",
    "let observationSize = Int(env.env.observation_space.shape[0])!\n",
    "let actionCount = Int(env.env.action_space.n)!\n",
    "var meanRewards: [Float] = []\n",
    "\n",
    "var net = Net(observationSize: Int(observationSize), hiddenSize: hiddenSize, actionCount: actionCount)\n",
    "let optimizer = Adam(for: net, learningRate: 0.01)\n",
    "var batchIndex = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNyB7mv3WvYz"
   },
   "source": [
    "### 训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUpqlZTLx0LX"
   },
   "outputs": [],
   "source": [
    "while true {\n",
    "    print(\"Processing mini batch \\(batchIndex)\")\n",
    "    batchIndex += 1\n",
    "\n",
    "    let episodes = nextBatch(env: env, net: net, batchSize: batchSize, actionCount: actionCount)\n",
    "    let (input, target, episodeCount, meanReward) = filteringBatch(\n",
    "      episodes: episodes, actionCount: actionCount)\n",
    "\n",
    "    let gradients = withLearningPhase(.training) {\n",
    "        net.gradient { net -> Tensor<Float> in\n",
    "            let logits = net(input)\n",
    "            let loss = softmaxCrossEntropy(logits: logits, probabilities: target)\n",
    "            print(\"loss is \\(loss)\")\n",
    "            return loss\n",
    "        }\n",
    "    }\n",
    "    optimizer.update(&net.allDifferentiableVariables, along: gradients)\n",
    "\n",
    "    print(\"It has episode count \\(episodeCount) and mean reward \\(meanReward)\")\n",
    "    meanRewards.append(meanReward)\n",
    "\n",
    "    if meanReward > 199 {\n",
    "        print(\"Solved\")\n",
    "        break\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "np3szYjSY0II"
   },
   "source": [
    "### 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOGyoyxlYyyW"
   },
   "outputs": [],
   "source": [
    "plt.plot(meanRewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EB9huPDde4n6"
   },
   "source": [
    "## 直接导入 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQDTY64he5DO"
   },
   "outputs": [],
   "source": [
    "import Glibc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRPjLQvAe-Eb"
   },
   "outputs": [],
   "source": [
    "let address = malloc(32)!\n",
    "let string = address.bindMemory(to: CChar.self, capacity: 32)\n",
    "\n",
    "// strcpy(string, \"Plain old C at Google I/O 2019!\")\n",
    "strcpy(string, \"我们可以直接写 C！\")\n",
    "puts(string)\n",
    "\n",
    "free(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo 2 - Interoperability.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
